{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0529 03:54:18.534895 4642352576 tokenization_utils.py:937] Model name 'BERT-pubmed-1000000-SQuAD' not found in model shortcut name list (bert-base-uncased, bert-large-uncased, bert-base-cased, bert-large-cased, bert-base-multilingual-uncased, bert-base-multilingual-cased, bert-base-chinese, bert-base-german-cased, bert-large-uncased-whole-word-masking, bert-large-cased-whole-word-masking, bert-large-uncased-whole-word-masking-finetuned-squad, bert-large-cased-whole-word-masking-finetuned-squad, bert-base-cased-finetuned-mrpc, bert-base-german-dbmdz-cased, bert-base-german-dbmdz-uncased, bert-base-finnish-cased-v1, bert-base-finnish-uncased-v1, bert-base-dutch-cased). Assuming 'BERT-pubmed-1000000-SQuAD' is a path, a model identifier, or url to a directory containing tokenizer files.\n",
      "I0529 03:54:18.536962 4642352576 tokenization_utils.py:966] Didn't find file BERT-pubmed-1000000-SQuAD/added_tokens.json. We won't load it.\n",
      "I0529 03:54:18.544423 4642352576 tokenization_utils.py:966] Didn't find file BERT-pubmed-1000000-SQuAD/special_tokens_map.json. We won't load it.\n",
      "I0529 03:54:18.545983 4642352576 tokenization_utils.py:966] Didn't find file BERT-pubmed-1000000-SQuAD/tokenizer_config.json. We won't load it.\n",
      "I0529 03:54:18.547883 4642352576 tokenization_utils.py:1021] loading file BERT-pubmed-1000000-SQuAD/vocab.txt\n",
      "I0529 03:54:18.549445 4642352576 tokenization_utils.py:1021] loading file None\n",
      "I0529 03:54:18.551053 4642352576 tokenization_utils.py:1021] loading file None\n",
      "I0529 03:54:18.556420 4642352576 tokenization_utils.py:1021] loading file None\n",
      "I0529 03:54:18.602168 4642352576 configuration_utils.py:283] loading configuration file BERT-pubmed-1000000-SQuAD/config.json\n",
      "I0529 03:54:18.603348 4642352576 configuration_utils.py:321] Model config BertConfig {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "I0529 03:54:18.604974 4642352576 modeling_utils.py:652] loading weights file BERT-pubmed-1000000-SQuAD/pytorch_model.bin\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertForQuestionAnswering\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "from transformers import AutoModel\n",
    "import torch\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('BERT-pubmed-1000000-SQuAD')\n",
    "model = BertForQuestionAnswering.from_pretrained(\"BERT-pubmed-1000000-SQuAD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def answer_question(question, answer_text):\n",
    "    '''\n",
    "    Takes a `question` string and an `answer_text` string (which contains the\n",
    "    answer), and identifies the words within the `answer_text` that are the\n",
    "    answer. Prints them out.\n",
    "    '''\n",
    "    # ======== Tokenize ========\n",
    "    # Apply the tokenizer to the input text, treating them as a text-pair.\n",
    "    input_ids = tokenizer.encode(question, answer_text)\n",
    "\n",
    "    # Report how long the input sequence is.\n",
    "    print('Query has {:,} tokens.\\n'.format(len(input_ids)))\n",
    "\n",
    "    # ======== Set Segment IDs ========\n",
    "    # Search the input_ids for the first instance of the `[SEP]` token.\n",
    "    sep_index = input_ids.index(tokenizer.sep_token_id)\n",
    "\n",
    "    # The number of segment A tokens includes the [SEP] token istelf.\n",
    "    num_seg_a = sep_index + 1\n",
    "\n",
    "    # The remainder are segment B.\n",
    "    num_seg_b = len(input_ids) - num_seg_a\n",
    "\n",
    "    # Construct the list of 0s and 1s.\n",
    "    segment_ids = [0]*num_seg_a + [1]*num_seg_b\n",
    "\n",
    "    # There should be a segment_id for every input token.\n",
    "    assert len(segment_ids) == len(input_ids)\n",
    "\n",
    "    # ======== Evaluate ========\n",
    "    # Run our example question through the model.\n",
    "    start_scores, end_scores = model(torch.tensor([input_ids]), # The tokens representing our input text.\n",
    "                                    token_type_ids=torch.tensor([segment_ids])) # The segment IDs to differentiate question from answer_text\n",
    "\n",
    "    \n",
    "    # ======== Reconstruct Answer ========\n",
    "    # Find the tokens with the highest `start` and `end` scores.\n",
    "    answer_start = torch.argmax(start_scores)\n",
    "    answer_end = torch.argmax(end_scores)\n",
    "    \n",
    "    # Get the string versions of the input tokens.\n",
    "    tokens = tokenizer.convert_ids_to_tokens(input_ids)\n",
    "    \n",
    "    print(len(start_scores.detach().numpy().flatten()))\n",
    "    print(len(tokens))\n",
    "\n",
    "    # Start with the first token.\n",
    "    answer = tokens[answer_start]\n",
    "    \n",
    "    # Select the remaining answer tokens and join them with whitespace.\n",
    "    for i in range(answer_start + 1, answer_end + 1):\n",
    "        # If it's a subword token, then recombine it with the previous token.\n",
    "        if tokens[i][0:2] == '##':\n",
    "            answer += tokens[i][2:]\n",
    "        \n",
    "        # Otherwise, add a space then the token.\n",
    "        else:\n",
    "            answer += ' ' + tokens[i]\n",
    "\n",
    "    print('Answer: \"' + answer + '\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Attending:[**First Name3 (LF) 148**] Chief Complaint: Abdominal Pain  History\n",
      "of Present Illness: This 48-year-old [**First Name3 (LF) 1229**] has mental\n",
      "retardation and a seizure syndrome. He presents to our emergency room acutely\n",
      "with reports of [**1-15**] days of abdominal pain as described by his\n",
      "caretakers, who find him grimacing in an umbilical position. He had a change in\n",
      "bowel habits and decreased PO intake for 2 weeks. He is largely unresponsive and\n",
      "he responds only to keep the stimulation for pain. He has had fevers for the\n",
      "last few days, up as high as 104 degrees. A workup was performed for this, and\n",
      "initial imaging of the abdomen showed multiple views consistent with a free air\n",
      "in the abdomen. This with a lactic acidosis, distended abdomen and a\n",
      "neutrophilia band shift, along with the after mentioned history was very\n",
      "concerning for an acute process which required an emergent operation. This was\n",
      "especially so given the fact that we could not adequately communicate with this\n",
      "[**Name2 (NI) 1229**] and did not know the full extent of his recognition of\n",
      "pain due to his mental retardation.\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "\n",
    "# Wrap text to 80 characters.\n",
    "wrapper = textwrap.TextWrapper(width=80) \n",
    "\n",
    "bert_abstract = '''\n",
    "Attending:[**First Name3 (LF) 148**]\n",
    "Chief Complaint:\n",
    "Abdominal Pain\n",
    "\n",
    "History of Present Illness:\n",
    "This 48-year-old [**First Name3 (LF) 1229**] has mental retardation and a seizure\n",
    "syndrome. He presents to our emergency room acutely with reports\n",
    "of [**1-15**] days of abdominal\n",
    "pain as described by his caretakers, who find him grimacing in\n",
    "an umbilical position. He had a change in bowel habits and\n",
    "decreased PO intake for 2 weeks. He is largely unresponsive and\n",
    "he responds only to keep the stimulation for pain. He has had\n",
    "fevers for the last few days, up as high as 104 degrees. A\n",
    "workup was performed for this, and initial imaging of the\n",
    "abdomen showed multiple views consistent with a free air in the\n",
    "abdomen. This with a lactic acidosis, distended abdomen and a\n",
    "neutrophilia band shift, along with the after mentioned history\n",
    "was very concerning for an acute process which\n",
    "required an emergent operation. This was especially so given the\n",
    "fact that we could not adequately communicate with this\n",
    "[**Name2 (NI) 1229**] and did not know the full extent of his recognition of\n",
    "pain due to his mental retardation.\n",
    "'''\n",
    "\n",
    "\n",
    "print(wrapper.fill(bert_abstract))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"What is chief complaint of a patient?\"\n",
    "\n",
    "answer_question(question, bert_abstract)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
